{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk.data\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(name,nrows = None):\n",
    "    datasets = {\n",
    "        'unlabeled_train': 'unlabeledTrainData.tsv',\n",
    "        'labeled_train': 'labeledTrainData.tsv',\n",
    "        'test': 'testData.tsv'\n",
    "    }\n",
    "    if name not in datasets:\n",
    "        raise ValueError(name)\n",
    "    data_file = os.path.join('.', 'data', datasets[name])\n",
    "    df = pd.read_csv(data_file, sep='\\t', escapechar='\\\\', nrows=nrows)\n",
    "    print('Number of reviews: {}'.format(len(df)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用无标签的数据和有标签的数据合在一起来建立word2vec模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入无标签数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9999_0</td>\n",
       "      <td>Watching Time Chasers, it obvious that it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45057_0</td>\n",
       "      <td>I saw this film about 20 years ago and remembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15561_0</td>\n",
       "      <td>Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7161_0</td>\n",
       "      <td>I went to see this film with a great deal of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43971_0</td>\n",
       "      <td>Yes, I agree with everyone on this site this m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             review\n",
       "0   9999_0  Watching Time Chasers, it obvious that it was ...\n",
       "1  45057_0  I saw this film about 20 years ago and remembe...\n",
       "2  15561_0  Minor Spoilers<br /><br />In New York, Joan Ba...\n",
       "3   7161_0  I went to see this film with a great deal of e...\n",
       "4  43971_0  Yes, I agree with everyone on this site this m..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = load_data('unlabeled_train')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清洗数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_stopwords = {}.fromkeys([line.rstrip() for line in open('./stopwords.txt')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 清理文本数据的方法\n",
    "def clean_text(text,remove_stopwords=False):\n",
    "    text = BeautifulSoup(text,'html.parser').get_text()\n",
    "    text = re.sub(r'[^a-zA-Z]',' ',text)\n",
    "    words = text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if w not in eng_stopwords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 打印函数\n",
    "def print_call_counts(f):\n",
    "    n = 0\n",
    "    def wrapped(*args,**kwargs):\n",
    "        \n",
    "        n +=1\n",
    "        if n%1000 == 1:\n",
    "            print 'method {} called {} times'.format(f.__name__, n)\n",
    "        return f(*args, **kwargs)\n",
    "    return wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 切割评论，看看有多少句子\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# @print_call_counts\n",
    "def split_sentence(review):\n",
    "    review = BeautifulSoup(review,'html.parser').get_text()# 先把文本中的网页标签给去掉\n",
    "    raw_sentence = tokenizer.tokenize(review.strip())      # 切分句子\n",
    "    sentences = [clean_text(s) for s in raw_sentence if s] # 清洗文本\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:219: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:219: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 reviews -> 539886sentences\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "sentences = sum(df.review.apply(split_sentence),[])\n",
    "print '{} reviews -> {}sentences'.format(len(df),len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解释一下上面的sum(df.review.apply(split_sentence),[]),它是吧一个二维list(这里是dataframe)转化为一维<br>\n",
    "所以这个的sentences就是一维的list，可以直接放入word2vec里进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'e', 'b', 'f', 'c', 'g', 'h']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一个例子,把二维列表转化为一维\n",
    "a = [['a','e'],['b','f'],['c','g','h']]\n",
    "b = sum(a,[])\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用gensim训练词嵌入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level = logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300features_10minwords_10context.model\n"
     ]
    }
   ],
   "source": [
    "# 设定词向量的参数\n",
    "num_features = 300  # 词向量的维度，也就是神经网络的\n",
    "min_word_count = 10 # 最小词频,默认是5，也就是至少出现5词以上\n",
    "num_workers = 4     # 执行的线程数\n",
    "context = 10        # 文本窗的大小\n",
    "downsampling = 1e-3 # 对频繁出现的词下采样的设置\n",
    "\n",
    "# 设置一个model_name 方便后面保存模型\n",
    "model_name = '{}features_{}minwords_{}context.model'.format(num_features, min_word_count, context)\n",
    "print model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-29 12:42:32,105 : INFO : collecting all words and their counts\n",
      "2018-03-29 12:42:32,107 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-03-29 12:42:32,219 : INFO : PROGRESS: at sentence #10000, processed 224745 words, keeping 17228 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-29 12:42:32,354 : INFO : PROGRESS: at sentence #20000, processed 441641 words, keeping 24512 word types\n",
      "2018-03-29 12:42:32,468 : INFO : PROGRESS: at sentence #30000, processed 663543 words, keeping 29701 word types\n",
      "2018-03-29 12:42:32,596 : INFO : PROGRESS: at sentence #40000, processed 883553 words, keeping 33883 word types\n",
      "2018-03-29 12:42:32,705 : INFO : PROGRESS: at sentence #50000, processed 1100419 words, keeping 37458 word types\n",
      "2018-03-29 12:42:32,824 : INFO : PROGRESS: at sentence #60000, processed 1322904 words, keeping 40681 word types\n",
      "2018-03-29 12:42:32,946 : INFO : PROGRESS: at sentence #70000, processed 1545641 words, keeping 43559 word types\n",
      "2018-03-29 12:42:33,061 : INFO : PROGRESS: at sentence #80000, processed 1765999 words, keeping 46082 word types\n",
      "2018-03-29 12:42:33,190 : INFO : PROGRESS: at sentence #90000, processed 1980415 words, keeping 48272 word types\n",
      "2018-03-29 12:42:33,300 : INFO : PROGRESS: at sentence #100000, processed 2203134 words, keeping 50499 word types\n",
      "2018-03-29 12:42:33,418 : INFO : PROGRESS: at sentence #110000, processed 2427570 words, keeping 52683 word types\n",
      "2018-03-29 12:42:33,531 : INFO : PROGRESS: at sentence #120000, processed 2647290 words, keeping 54758 word types\n",
      "2018-03-29 12:42:33,653 : INFO : PROGRESS: at sentence #130000, processed 2868377 words, keeping 56547 word types\n",
      "2018-03-29 12:42:33,766 : INFO : PROGRESS: at sentence #140000, processed 3086466 words, keeping 58260 word types\n",
      "2018-03-29 12:42:33,879 : INFO : PROGRESS: at sentence #150000, processed 3303478 words, keeping 59916 word types\n",
      "2018-03-29 12:42:33,996 : INFO : PROGRESS: at sentence #160000, processed 3522190 words, keeping 61597 word types\n",
      "2018-03-29 12:42:34,115 : INFO : PROGRESS: at sentence #170000, processed 3745059 words, keeping 63206 word types\n",
      "2018-03-29 12:42:34,240 : INFO : PROGRESS: at sentence #180000, processed 3963516 words, keeping 64736 word types\n",
      "2018-03-29 12:42:34,364 : INFO : PROGRESS: at sentence #190000, processed 4186546 words, keeping 66278 word types\n",
      "2018-03-29 12:42:34,485 : INFO : PROGRESS: at sentence #200000, processed 4411812 words, keeping 67812 word types\n",
      "2018-03-29 12:42:34,606 : INFO : PROGRESS: at sentence #210000, processed 4635620 words, keeping 69148 word types\n",
      "2018-03-29 12:42:34,716 : INFO : PROGRESS: at sentence #220000, processed 4853028 words, keeping 70451 word types\n",
      "2018-03-29 12:42:34,830 : INFO : PROGRESS: at sentence #230000, processed 5073040 words, keeping 71811 word types\n",
      "2018-03-29 12:42:34,936 : INFO : PROGRESS: at sentence #240000, processed 5289935 words, keeping 73115 word types\n",
      "2018-03-29 12:42:35,046 : INFO : PROGRESS: at sentence #250000, processed 5509836 words, keeping 74320 word types\n",
      "2018-03-29 12:42:35,161 : INFO : PROGRESS: at sentence #260000, processed 5730023 words, keeping 75576 word types\n",
      "2018-03-29 12:42:35,276 : INFO : PROGRESS: at sentence #270000, processed 5949970 words, keeping 76705 word types\n",
      "2018-03-29 12:42:35,399 : INFO : PROGRESS: at sentence #280000, processed 6168539 words, keeping 77850 word types\n",
      "2018-03-29 12:42:35,521 : INFO : PROGRESS: at sentence #290000, processed 6390722 words, keeping 79021 word types\n",
      "2018-03-29 12:42:35,639 : INFO : PROGRESS: at sentence #300000, processed 6608140 words, keeping 80131 word types\n",
      "2018-03-29 12:42:35,774 : INFO : PROGRESS: at sentence #310000, processed 6831575 words, keeping 81172 word types\n",
      "2018-03-29 12:42:35,894 : INFO : PROGRESS: at sentence #320000, processed 7048962 words, keeping 82292 word types\n",
      "2018-03-29 12:42:36,008 : INFO : PROGRESS: at sentence #330000, processed 7270204 words, keeping 83389 word types\n",
      "2018-03-29 12:42:36,125 : INFO : PROGRESS: at sentence #340000, processed 7488192 words, keeping 84346 word types\n",
      "2018-03-29 12:42:36,241 : INFO : PROGRESS: at sentence #350000, processed 7705283 words, keeping 85436 word types\n",
      "2018-03-29 12:42:36,351 : INFO : PROGRESS: at sentence #360000, processed 7925805 words, keeping 86428 word types\n",
      "2018-03-29 12:42:36,472 : INFO : PROGRESS: at sentence #370000, processed 8145458 words, keeping 87354 word types\n",
      "2018-03-29 12:42:36,590 : INFO : PROGRESS: at sentence #380000, processed 8362212 words, keeping 88364 word types\n",
      "2018-03-29 12:42:36,703 : INFO : PROGRESS: at sentence #390000, processed 8583417 words, keeping 89357 word types\n",
      "2018-03-29 12:42:36,811 : INFO : PROGRESS: at sentence #400000, processed 8800756 words, keeping 90304 word types\n",
      "2018-03-29 12:42:36,925 : INFO : PROGRESS: at sentence #410000, processed 9020084 words, keeping 91206 word types\n",
      "2018-03-29 12:42:37,023 : INFO : PROGRESS: at sentence #420000, processed 9241830 words, keeping 92123 word types\n",
      "2018-03-29 12:42:37,137 : INFO : PROGRESS: at sentence #430000, processed 9457373 words, keeping 92971 word types\n",
      "2018-03-29 12:42:37,232 : INFO : PROGRESS: at sentence #440000, processed 9679972 words, keeping 93938 word types\n",
      "2018-03-29 12:42:37,331 : INFO : PROGRESS: at sentence #450000, processed 9900316 words, keeping 94839 word types\n",
      "2018-03-29 12:42:37,431 : INFO : PROGRESS: at sentence #460000, processed 10124470 words, keeping 95649 word types\n",
      "2018-03-29 12:42:37,534 : INFO : PROGRESS: at sentence #470000, processed 10342082 words, keeping 96507 word types\n",
      "2018-03-29 12:42:37,621 : INFO : PROGRESS: at sentence #480000, processed 10559354 words, keeping 97329 word types\n",
      "2018-03-29 12:42:37,721 : INFO : PROGRESS: at sentence #490000, processed 10777572 words, keeping 98125 word types\n",
      "2018-03-29 12:42:37,822 : INFO : PROGRESS: at sentence #500000, processed 10993482 words, keeping 98944 word types\n",
      "2018-03-29 12:42:37,908 : INFO : PROGRESS: at sentence #510000, processed 11211315 words, keeping 99766 word types\n",
      "2018-03-29 12:42:38,019 : INFO : PROGRESS: at sentence #520000, processed 11435370 words, keeping 100643 word types\n",
      "2018-03-29 12:42:38,117 : INFO : PROGRESS: at sentence #530000, processed 11660884 words, keeping 101446 word types\n",
      "2018-03-29 12:42:38,234 : INFO : collected 102302 word types from a corpus of 11876777 raw words and 539886 sentences\n",
      "2018-03-29 12:42:38,236 : INFO : Loading a fresh vocabulary\n",
      "2018-03-29 12:42:38,458 : INFO : min_count=10 retains 28314 unique words (27% of original 102302, drops 73988)\n",
      "2018-03-29 12:42:38,459 : INFO : min_count=10 leaves 11700560 word corpus (98% of original 11876777, drops 176217)\n",
      "2018-03-29 12:42:38,554 : INFO : deleting the raw counts dictionary of 102302 items\n",
      "2018-03-29 12:42:38,559 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-03-29 12:42:38,561 : INFO : downsampling leaves estimated 8728648 word corpus (74.6% of prior 11700560)\n",
      "2018-03-29 12:42:38,680 : INFO : estimated required memory for 28314 words and 300 dimensions: 82110600 bytes\n",
      "2018-03-29 12:42:38,682 : INFO : resetting layer weights\n",
      "2018-03-29 12:42:39,153 : INFO : training model with 4 workers on 28314 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-03-29 12:42:40,171 : INFO : EPOCH 1 - PROGRESS: at 7.96% examples, 689791 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:42:41,182 : INFO : EPOCH 1 - PROGRESS: at 16.28% examples, 704238 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:42:42,186 : INFO : EPOCH 1 - PROGRESS: at 24.54% examples, 710534 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:42:43,207 : INFO : EPOCH 1 - PROGRESS: at 32.98% examples, 712533 words/s, in_qsize 7, out_qsize 1\n",
      "2018-03-29 12:42:44,209 : INFO : EPOCH 1 - PROGRESS: at 41.39% examples, 717845 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:42:45,222 : INFO : EPOCH 1 - PROGRESS: at 49.84% examples, 718988 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:42:46,227 : INFO : EPOCH 1 - PROGRESS: at 57.64% examples, 713281 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-29 12:42:47,234 : INFO : EPOCH 1 - PROGRESS: at 65.82% examples, 712489 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:42:48,244 : INFO : EPOCH 1 - PROGRESS: at 73.70% examples, 708397 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:42:49,250 : INFO : EPOCH 1 - PROGRESS: at 81.59% examples, 706197 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-29 12:42:50,250 : INFO : EPOCH 1 - PROGRESS: at 89.08% examples, 701359 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-29 12:42:51,265 : INFO : EPOCH 1 - PROGRESS: at 96.92% examples, 698907 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-29 12:42:51,642 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-29 12:42:51,657 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-29 12:42:51,659 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-29 12:42:51,667 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-29 12:42:51,668 : INFO : EPOCH - 1 : training on 11876777 raw words (8730625 effective words) took 12.5s, 698007 effective words/s\n",
      "2018-03-29 12:42:52,675 : INFO : EPOCH 2 - PROGRESS: at 7.45% examples, 651255 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-29 12:42:53,680 : INFO : EPOCH 2 - PROGRESS: at 15.06% examples, 657825 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-29 12:42:54,683 : INFO : EPOCH 2 - PROGRESS: at 23.29% examples, 677544 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-29 12:42:55,699 : INFO : EPOCH 2 - PROGRESS: at 31.64% examples, 686783 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-29 12:42:56,710 : INFO : EPOCH 2 - PROGRESS: at 39.87% examples, 693093 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:42:57,720 : INFO : EPOCH 2 - PROGRESS: at 48.33% examples, 698667 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-29 12:42:58,739 : INFO : EPOCH 2 - PROGRESS: at 56.40% examples, 698238 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:42:59,740 : INFO : EPOCH 2 - PROGRESS: at 65.08% examples, 704796 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:00,748 : INFO : EPOCH 2 - PROGRESS: at 73.36% examples, 705753 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:01,752 : INFO : EPOCH 2 - PROGRESS: at 81.76% examples, 708187 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:02,760 : INFO : EPOCH 2 - PROGRESS: at 89.92% examples, 707972 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:03,770 : INFO : EPOCH 2 - PROGRESS: at 98.42% examples, 710095 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:03,927 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-29 12:43:03,932 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-29 12:43:03,938 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-29 12:43:03,940 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-29 12:43:03,941 : INFO : EPOCH - 2 : training on 11876777 raw words (8728399 effective words) took 12.3s, 711475 effective words/s\n",
      "2018-03-29 12:43:04,958 : INFO : EPOCH 3 - PROGRESS: at 8.12% examples, 706793 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:05,968 : INFO : EPOCH 3 - PROGRESS: at 16.36% examples, 707669 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-29 12:43:06,970 : INFO : EPOCH 3 - PROGRESS: at 25.23% examples, 730271 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:07,980 : INFO : EPOCH 3 - PROGRESS: at 33.06% examples, 716586 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:08,983 : INFO : EPOCH 3 - PROGRESS: at 41.39% examples, 719369 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:10,009 : INFO : EPOCH 3 - PROGRESS: at 49.84% examples, 718754 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:11,015 : INFO : EPOCH 3 - PROGRESS: at 58.30% examples, 721340 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-29 12:43:12,018 : INFO : EPOCH 3 - PROGRESS: at 66.84% examples, 723531 words/s, in_qsize 6, out_qsize 0\n",
      "2018-03-29 12:43:13,021 : INFO : EPOCH 3 - PROGRESS: at 75.05% examples, 721961 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:14,023 : INFO : EPOCH 3 - PROGRESS: at 83.45% examples, 722988 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-29 12:43:15,029 : INFO : EPOCH 3 - PROGRESS: at 91.90% examples, 723530 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:15,968 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-29 12:43:15,977 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-29 12:43:15,988 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-29 12:43:15,989 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-29 12:43:15,990 : INFO : EPOCH - 3 : training on 11876777 raw words (8728677 effective words) took 12.0s, 724764 effective words/s\n",
      "2018-03-29 12:43:16,999 : INFO : EPOCH 4 - PROGRESS: at 8.21% examples, 716157 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-29 12:43:18,013 : INFO : EPOCH 4 - PROGRESS: at 16.72% examples, 723474 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:19,013 : INFO : EPOCH 4 - PROGRESS: at 25.05% examples, 726567 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:20,031 : INFO : EPOCH 4 - PROGRESS: at 33.41% examples, 723364 words/s, in_qsize 7, out_qsize 1\n",
      "2018-03-29 12:43:21,032 : INFO : EPOCH 4 - PROGRESS: at 41.55% examples, 722271 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:22,037 : INFO : EPOCH 4 - PROGRESS: at 50.08% examples, 724719 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:23,042 : INFO : EPOCH 4 - PROGRESS: at 58.57% examples, 726654 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-29 12:43:24,051 : INFO : EPOCH 4 - PROGRESS: at 66.76% examples, 724052 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:25,058 : INFO : EPOCH 4 - PROGRESS: at 75.30% examples, 725471 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:26,059 : INFO : EPOCH 4 - PROGRESS: at 83.72% examples, 726022 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:27,074 : INFO : EPOCH 4 - PROGRESS: at 92.06% examples, 725073 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-29 12:43:27,976 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-29 12:43:27,980 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-29 12:43:27,989 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-29 12:43:27,991 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-29 12:43:27,993 : INFO : EPOCH - 4 : training on 11876777 raw words (8727587 effective words) took 12.0s, 727466 effective words/s\n",
      "2018-03-29 12:43:29,002 : INFO : EPOCH 5 - PROGRESS: at 8.30% examples, 722654 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-29 12:43:30,008 : INFO : EPOCH 5 - PROGRESS: at 17.05% examples, 740504 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-29 12:43:31,016 : INFO : EPOCH 5 - PROGRESS: at 25.65% examples, 743492 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:32,022 : INFO : EPOCH 5 - PROGRESS: at 34.65% examples, 752656 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:33,030 : INFO : EPOCH 5 - PROGRESS: at 43.52% examples, 756512 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-29 12:43:34,040 : INFO : EPOCH 5 - PROGRESS: at 52.29% examples, 756368 words/s, in_qsize 7, out_qsize 1\n",
      "2018-03-29 12:43:35,052 : INFO : EPOCH 5 - PROGRESS: at 61.26% examples, 759240 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:36,059 : INFO : EPOCH 5 - PROGRESS: at 69.98% examples, 758062 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-29 12:43:37,063 : INFO : EPOCH 5 - PROGRESS: at 78.83% examples, 759173 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:38,079 : INFO : EPOCH 5 - PROGRESS: at 87.79% examples, 760462 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-29 12:43:39,090 : INFO : EPOCH 5 - PROGRESS: at 96.76% examples, 761230 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-29 12:43:39,418 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-29 12:43:39,430 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-29 12:43:39,435 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-29 12:43:39,437 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-29 12:43:39,439 : INFO : EPOCH - 5 : training on 11876777 raw words (8729994 effective words) took 11.4s, 762924 effective words/s\n",
      "2018-03-29 12:43:39,440 : INFO : training on a 59383885 raw words (43645282 effective words) took 60.3s, 723972 effective words/s\n",
      "2018-03-29 12:43:39,442 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-03-29 12:43:39,712 : INFO : saving Word2Vec object under ./model/300features_10minwords_10context.model, separately None\n",
      "2018-03-29 12:43:39,713 : INFO : not storing attribute vectors_norm\n",
      "2018-03-29 12:43:39,715 : INFO : not storing attribute cum_table\n",
      "2018-03-29 12:43:40,064 : INFO : saved ./model/300features_10minwords_10context.model\n"
     ]
    }
   ],
   "source": [
    "print('Traing model...')\n",
    "# 这里的sentences要求是一个一维List\n",
    "model = word2vec.Word2Vec(sentences,workers=num_workers,\n",
    "                 size=num_features,min_count=min_word_count,\n",
    "                 window=context,sample = downsampling\n",
    "                )\n",
    "# 初始化权重（映射矩阵）\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# 存储模型，方便以后使用\n",
    "model.save(os.path.join('.','model',model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 看看词向量训练的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitchen\n",
      "berlin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"man woman child kitchen\".split()))\n",
    "print(model.doesnt_match('france england germany berlin'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'mouse', 0.736385703086853),\n",
       " (u'sheep', 0.6904305219650269),\n",
       " (u'dog', 0.6760936975479126),\n",
       " (u'rabbit', 0.6244864463806152),\n",
       " (u'doll', 0.5945224761962891),\n",
       " (u'monkey', 0.5916447043418884),\n",
       " (u'demon', 0.5911267995834351),\n",
       " (u'bird', 0.5907045602798462),\n",
       " (u'rat', 0.5844168066978455),\n",
       " (u'clown', 0.5842093229293823)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'fbi', 0.6906628012657166),\n",
       " (u'authorities', 0.6834771037101746),\n",
       " (u'investigating', 0.6793107986450195),\n",
       " (u'federal', 0.6773560047149658),\n",
       " (u'officer', 0.6663809418678284),\n",
       " (u'cops', 0.6512453556060791),\n",
       " (u'homicide', 0.6491827368736267),\n",
       " (u'officers', 0.6399737000465393),\n",
       " (u'cia', 0.6375499367713928),\n",
       " (u'agents', 0.6364037990570068)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('police')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7508594107389206"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('women','men')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
