{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk.data\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(name,nrows = None):\n",
    "    datasets = {\n",
    "        'unlabeled_train': 'unlabeledTrainData.tsv',\n",
    "        'labeled_train': 'labeledTrainData.tsv',\n",
    "        'test': 'testData.tsv'\n",
    "    }\n",
    "    if name not in datasets:\n",
    "        raise ValueError(name)\n",
    "    data_file = os.path.join('.', 'data', datasets[name])\n",
    "    df = pd.read_csv(data_file, sep='\\t', escapechar='\\\\', nrows=nrows)\n",
    "    print('Number of reviews: {}'.format(len(df)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用无标签的数据和有标签的数据合在一起来建立word2vec模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入无标签数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9999_0</td>\n",
       "      <td>Watching Time Chasers, it obvious that it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45057_0</td>\n",
       "      <td>I saw this film about 20 years ago and remembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15561_0</td>\n",
       "      <td>Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7161_0</td>\n",
       "      <td>I went to see this film with a great deal of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43971_0</td>\n",
       "      <td>Yes, I agree with everyone on this site this m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             review\n",
       "0   9999_0  Watching Time Chasers, it obvious that it was ...\n",
       "1  45057_0  I saw this film about 20 years ago and remembe...\n",
       "2  15561_0  Minor Spoilers<br /><br />In New York, Joan Ba...\n",
       "3   7161_0  I went to see this film with a great deal of e...\n",
       "4  43971_0  Yes, I agree with everyone on this site this m..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = load_data('unlabeled_train')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清洗数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_stopwords = {}.fromkeys([line.rstrip() for line in open('./stopwords.txt')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理文本数据的方法\n",
    "def clean_text(text,remove_stopwords=False):\n",
    "    text = BeautifulSoup(text,'html.parser').get_text()\n",
    "    text = re.sub(r'[^a-zA-Z]',' ',text)\n",
    "    words = text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if w not in eng_stopwords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印函数\n",
    "def print_call_counts(f):\n",
    "    n = 0\n",
    "    def wrapped(*args,**kwargs):\n",
    "        \n",
    "        n +=1\n",
    "        if n%1000 == 1:\n",
    "            print 'method {} called {} times'.format(f.__name__, n)\n",
    "        return f(*args, **kwargs)\n",
    "    return wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 切割评论，看看有多少句子\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "@print_call_counts\n",
    "def split_sentence(review):\n",
    "    review = BeautifulSoup(review,'html.parser').get_text()# 先把文本中的网页标签给去掉\n",
    "    raw_sentence = tokenizer.tokenize(review.strip())      # 切分句子\n",
    "    sentences = [clean_text(s) for s in raw_sentence if s] # 清洗文本\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n",
      "50000 reviews -> 539886sentences\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "sentences = sum(df.review.apply(split_sentence),[])\n",
    "print '{} reviews -> {}sentences'.format(len(df),len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用gensim训练词嵌入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level = logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 设定词向量的参数\n",
    "num_features = 300  # 词向量的维度，也就是神经网络的\n",
    "min_word_count = 10 # 最小词频,默认是5，也就是至少出现5词以上\n",
    "num_workers = 4     # 执行的线程数\n",
    "context = 10        # 文本窗的大小\n",
    "downsampling = 1e-3 # 对频繁出现的词下采样的设置\n",
    "\n",
    "model_name = '{}features_{}minwords_{}context.model'.format(num_features, min_word_count, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-28 21:53:10,212 : INFO : collecting all words and their counts\n",
      "2018-03-28 21:53:10,215 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-03-28 21:53:10,350 : INFO : PROGRESS: at sentence #10000, processed 224745 words, keeping 17228 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-28 21:53:10,494 : INFO : PROGRESS: at sentence #20000, processed 441641 words, keeping 24512 word types\n",
      "2018-03-28 21:53:10,618 : INFO : PROGRESS: at sentence #30000, processed 663543 words, keeping 29701 word types\n",
      "2018-03-28 21:53:10,752 : INFO : PROGRESS: at sentence #40000, processed 883553 words, keeping 33883 word types\n",
      "2018-03-28 21:53:10,867 : INFO : PROGRESS: at sentence #50000, processed 1100419 words, keeping 37458 word types\n",
      "2018-03-28 21:53:10,994 : INFO : PROGRESS: at sentence #60000, processed 1322904 words, keeping 40681 word types\n",
      "2018-03-28 21:53:11,114 : INFO : PROGRESS: at sentence #70000, processed 1545641 words, keeping 43559 word types\n",
      "2018-03-28 21:53:11,245 : INFO : PROGRESS: at sentence #80000, processed 1765999 words, keeping 46082 word types\n",
      "2018-03-28 21:53:11,378 : INFO : PROGRESS: at sentence #90000, processed 1980415 words, keeping 48272 word types\n",
      "2018-03-28 21:53:11,515 : INFO : PROGRESS: at sentence #100000, processed 2203134 words, keeping 50499 word types\n",
      "2018-03-28 21:53:11,649 : INFO : PROGRESS: at sentence #110000, processed 2427570 words, keeping 52683 word types\n",
      "2018-03-28 21:53:11,787 : INFO : PROGRESS: at sentence #120000, processed 2647290 words, keeping 54758 word types\n",
      "2018-03-28 21:53:11,940 : INFO : PROGRESS: at sentence #130000, processed 2868377 words, keeping 56547 word types\n",
      "2018-03-28 21:53:12,079 : INFO : PROGRESS: at sentence #140000, processed 3086466 words, keeping 58260 word types\n",
      "2018-03-28 21:53:12,208 : INFO : PROGRESS: at sentence #150000, processed 3303478 words, keeping 59916 word types\n",
      "2018-03-28 21:53:12,337 : INFO : PROGRESS: at sentence #160000, processed 3522190 words, keeping 61597 word types\n",
      "2018-03-28 21:53:12,473 : INFO : PROGRESS: at sentence #170000, processed 3745059 words, keeping 63206 word types\n",
      "2018-03-28 21:53:12,613 : INFO : PROGRESS: at sentence #180000, processed 3963516 words, keeping 64736 word types\n",
      "2018-03-28 21:53:12,755 : INFO : PROGRESS: at sentence #190000, processed 4186546 words, keeping 66278 word types\n",
      "2018-03-28 21:53:12,889 : INFO : PROGRESS: at sentence #200000, processed 4411812 words, keeping 67812 word types\n",
      "2018-03-28 21:53:13,034 : INFO : PROGRESS: at sentence #210000, processed 4635620 words, keeping 69148 word types\n",
      "2018-03-28 21:53:13,177 : INFO : PROGRESS: at sentence #220000, processed 4853028 words, keeping 70451 word types\n",
      "2018-03-28 21:53:13,305 : INFO : PROGRESS: at sentence #230000, processed 5073040 words, keeping 71811 word types\n",
      "2018-03-28 21:53:13,439 : INFO : PROGRESS: at sentence #240000, processed 5289935 words, keeping 73115 word types\n",
      "2018-03-28 21:53:13,578 : INFO : PROGRESS: at sentence #250000, processed 5509836 words, keeping 74320 word types\n",
      "2018-03-28 21:53:13,716 : INFO : PROGRESS: at sentence #260000, processed 5730023 words, keeping 75576 word types\n",
      "2018-03-28 21:53:13,858 : INFO : PROGRESS: at sentence #270000, processed 5949970 words, keeping 76705 word types\n",
      "2018-03-28 21:53:14,003 : INFO : PROGRESS: at sentence #280000, processed 6168539 words, keeping 77850 word types\n",
      "2018-03-28 21:53:14,133 : INFO : PROGRESS: at sentence #290000, processed 6390722 words, keeping 79021 word types\n",
      "2018-03-28 21:53:14,268 : INFO : PROGRESS: at sentence #300000, processed 6608140 words, keeping 80131 word types\n",
      "2018-03-28 21:53:14,415 : INFO : PROGRESS: at sentence #310000, processed 6831575 words, keeping 81172 word types\n",
      "2018-03-28 21:53:14,548 : INFO : PROGRESS: at sentence #320000, processed 7048962 words, keeping 82292 word types\n",
      "2018-03-28 21:53:14,684 : INFO : PROGRESS: at sentence #330000, processed 7270204 words, keeping 83389 word types\n",
      "2018-03-28 21:53:14,805 : INFO : PROGRESS: at sentence #340000, processed 7488192 words, keeping 84346 word types\n",
      "2018-03-28 21:53:14,933 : INFO : PROGRESS: at sentence #350000, processed 7705283 words, keeping 85436 word types\n",
      "2018-03-28 21:53:15,073 : INFO : PROGRESS: at sentence #360000, processed 7925805 words, keeping 86428 word types\n",
      "2018-03-28 21:53:15,206 : INFO : PROGRESS: at sentence #370000, processed 8145458 words, keeping 87354 word types\n",
      "2018-03-28 21:53:15,333 : INFO : PROGRESS: at sentence #380000, processed 8362212 words, keeping 88364 word types\n",
      "2018-03-28 21:53:15,461 : INFO : PROGRESS: at sentence #390000, processed 8583417 words, keeping 89357 word types\n",
      "2018-03-28 21:53:15,587 : INFO : PROGRESS: at sentence #400000, processed 8800756 words, keeping 90304 word types\n",
      "2018-03-28 21:53:15,733 : INFO : PROGRESS: at sentence #410000, processed 9020084 words, keeping 91206 word types\n",
      "2018-03-28 21:53:15,865 : INFO : PROGRESS: at sentence #420000, processed 9241830 words, keeping 92123 word types\n",
      "2018-03-28 21:53:15,995 : INFO : PROGRESS: at sentence #430000, processed 9457373 words, keeping 92971 word types\n",
      "2018-03-28 21:53:16,116 : INFO : PROGRESS: at sentence #440000, processed 9679972 words, keeping 93938 word types\n",
      "2018-03-28 21:53:16,239 : INFO : PROGRESS: at sentence #450000, processed 9900316 words, keeping 94839 word types\n",
      "2018-03-28 21:53:16,375 : INFO : PROGRESS: at sentence #460000, processed 10124470 words, keeping 95649 word types\n",
      "2018-03-28 21:53:16,515 : INFO : PROGRESS: at sentence #470000, processed 10342082 words, keeping 96507 word types\n",
      "2018-03-28 21:53:16,661 : INFO : PROGRESS: at sentence #480000, processed 10559354 words, keeping 97329 word types\n",
      "2018-03-28 21:53:16,803 : INFO : PROGRESS: at sentence #490000, processed 10777572 words, keeping 98125 word types\n",
      "2018-03-28 21:53:16,943 : INFO : PROGRESS: at sentence #500000, processed 10993482 words, keeping 98944 word types\n",
      "2018-03-28 21:53:17,078 : INFO : PROGRESS: at sentence #510000, processed 11211315 words, keeping 99766 word types\n",
      "2018-03-28 21:53:17,208 : INFO : PROGRESS: at sentence #520000, processed 11435370 words, keeping 100643 word types\n",
      "2018-03-28 21:53:17,332 : INFO : PROGRESS: at sentence #530000, processed 11660884 words, keeping 101446 word types\n",
      "2018-03-28 21:53:17,462 : INFO : collected 102302 word types from a corpus of 11876777 raw words and 539886 sentences\n",
      "2018-03-28 21:53:17,464 : INFO : Loading a fresh vocabulary\n",
      "2018-03-28 21:53:17,682 : INFO : min_count=40 retains 13056 unique words (12% of original 102302, drops 89246)\n",
      "2018-03-28 21:53:17,684 : INFO : min_count=40 leaves 11400302 word corpus (95% of original 11876777, drops 476475)\n",
      "2018-03-28 21:53:17,740 : INFO : deleting the raw counts dictionary of 102302 items\n",
      "2018-03-28 21:53:17,750 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-03-28 21:53:17,752 : INFO : downsampling leaves estimated 8394111 word corpus (73.6% of prior 11400302)\n",
      "2018-03-28 21:53:17,824 : INFO : estimated required memory for 13056 words and 300 dimensions: 37862400 bytes\n",
      "2018-03-28 21:53:17,827 : INFO : resetting layer weights\n",
      "2018-03-28 21:53:18,040 : INFO : training model with 4 workers on 13056 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-03-28 21:53:19,046 : INFO : EPOCH 1 - PROGRESS: at 9.24% examples, 776107 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-28 21:53:20,052 : INFO : EPOCH 1 - PROGRESS: at 19.55% examples, 819776 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:53:21,056 : INFO : EPOCH 1 - PROGRESS: at 29.39% examples, 820403 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-28 21:53:22,061 : INFO : EPOCH 1 - PROGRESS: at 39.53% examples, 829212 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:53:23,061 : INFO : EPOCH 1 - PROGRESS: at 47.57% examples, 797548 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-28 21:53:24,073 : INFO : EPOCH 1 - PROGRESS: at 56.82% examples, 793487 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-28 21:53:25,066 : INFO : EPOCH 1 - PROGRESS: at 66.17% examples, 791869 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-28 21:53:26,069 : INFO : EPOCH 1 - PROGRESS: at 76.48% examples, 800262 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:53:27,071 : INFO : EPOCH 1 - PROGRESS: at 86.80% examples, 807563 words/s, in_qsize 7, out_qsize 1\n",
      "2018-03-28 21:53:28,077 : INFO : EPOCH 1 - PROGRESS: at 97.34% examples, 814457 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-28 21:53:28,309 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-28 21:53:28,317 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-28 21:53:28,323 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-28 21:53:28,327 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-28 21:53:28,328 : INFO : EPOCH - 1 : training on 11876777 raw words (8395083 effective words) took 10.3s, 816439 effective words/s\n",
      "2018-03-28 21:53:29,345 : INFO : EPOCH 2 - PROGRESS: at 10.23% examples, 850388 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:53:30,348 : INFO : EPOCH 2 - PROGRESS: at 20.97% examples, 875399 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:53:31,357 : INFO : EPOCH 2 - PROGRESS: at 31.64% examples, 879125 words/s, in_qsize 6, out_qsize 0\n",
      "2018-03-28 21:53:32,363 : INFO : EPOCH 2 - PROGRESS: at 42.07% examples, 878408 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:53:33,372 : INFO : EPOCH 2 - PROGRESS: at 53.07% examples, 884919 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:53:34,384 : INFO : EPOCH 2 - PROGRESS: at 63.65% examples, 883649 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-28 21:53:35,394 : INFO : EPOCH 2 - PROGRESS: at 74.70% examples, 888247 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-28 21:53:36,403 : INFO : EPOCH 2 - PROGRESS: at 83.54% examples, 868960 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:53:37,414 : INFO : EPOCH 2 - PROGRESS: at 92.50% examples, 855331 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:53:38,255 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-28 21:53:38,261 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-28 21:53:38,271 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-28 21:53:38,273 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-28 21:53:38,274 : INFO : EPOCH - 2 : training on 11876777 raw words (8394728 effective words) took 9.9s, 844362 effective words/s\n",
      "2018-03-28 21:53:39,291 : INFO : EPOCH 3 - PROGRESS: at 8.47% examples, 711823 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-28 21:53:40,292 : INFO : EPOCH 3 - PROGRESS: at 18.80% examples, 788919 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:53:41,303 : INFO : EPOCH 3 - PROGRESS: at 29.64% examples, 828256 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:53:42,298 : INFO : EPOCH 3 - PROGRESS: at 40.23% examples, 844736 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:53:43,305 : INFO : EPOCH 3 - PROGRESS: at 50.58% examples, 847921 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:53:44,311 : INFO : EPOCH 3 - PROGRESS: at 60.16% examples, 839951 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:53:45,325 : INFO : EPOCH 3 - PROGRESS: at 68.36% examples, 817032 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-28 21:53:46,318 : INFO : EPOCH 3 - PROGRESS: at 77.81% examples, 813665 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:53:47,327 : INFO : EPOCH 3 - PROGRESS: at 86.80% examples, 806336 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:53:48,331 : INFO : EPOCH 3 - PROGRESS: at 94.96% examples, 793196 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-28 21:53:48,883 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-28 21:53:48,885 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-28 21:53:48,891 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-28 21:53:48,894 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-28 21:53:48,895 : INFO : EPOCH - 3 : training on 11876777 raw words (8394203 effective words) took 10.6s, 791450 effective words/s\n",
      "2018-03-28 21:53:49,919 : INFO : EPOCH 4 - PROGRESS: at 8.21% examples, 681064 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:53:50,925 : INFO : EPOCH 4 - PROGRESS: at 17.74% examples, 736672 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:53:51,926 : INFO : EPOCH 4 - PROGRESS: at 27.61% examples, 768273 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-28 21:53:52,931 : INFO : EPOCH 4 - PROGRESS: at 37.54% examples, 784755 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:53:53,933 : INFO : EPOCH 4 - PROGRESS: at 48.40% examples, 809388 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-28 21:53:54,935 : INFO : EPOCH 4 - PROGRESS: at 59.17% examples, 824714 words/s, in_qsize 6, out_qsize 0\n",
      "2018-03-28 21:53:55,944 : INFO : EPOCH 4 - PROGRESS: at 70.25% examples, 837782 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:53:56,950 : INFO : EPOCH 4 - PROGRESS: at 81.20% examples, 847241 words/s, in_qsize 7, out_qsize 1\n",
      "2018-03-28 21:53:57,950 : INFO : EPOCH 4 - PROGRESS: at 91.98% examples, 853269 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:53:58,685 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-28 21:53:58,697 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-28 21:53:58,698 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-28 21:53:58,709 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-28 21:53:58,710 : INFO : EPOCH - 4 : training on 11876777 raw words (8395076 effective words) took 9.8s, 856087 effective words/s\n",
      "2018-03-28 21:53:59,716 : INFO : EPOCH 5 - PROGRESS: at 10.64% examples, 894509 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:54:00,722 : INFO : EPOCH 5 - PROGRESS: at 21.38% examples, 896056 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:54:01,739 : INFO : EPOCH 5 - PROGRESS: at 32.40% examples, 900146 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:54:02,740 : INFO : EPOCH 5 - PROGRESS: at 43.33% examples, 905868 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:54:03,742 : INFO : EPOCH 5 - PROGRESS: at 54.03% examples, 903517 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-28 21:54:04,743 : INFO : EPOCH 5 - PROGRESS: at 64.66% examples, 901019 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:54:05,750 : INFO : EPOCH 5 - PROGRESS: at 74.29% examples, 886735 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:54:06,751 : INFO : EPOCH 5 - PROGRESS: at 83.27% examples, 870025 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-28 21:54:07,756 : INFO : EPOCH 5 - PROGRESS: at 93.64% examples, 868556 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-28 21:54:08,324 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-28 21:54:08,332 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-28 21:54:08,339 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-28 21:54:08,341 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-28 21:54:08,343 : INFO : EPOCH - 5 : training on 11876777 raw words (8395414 effective words) took 9.6s, 871901 effective words/s\n",
      "2018-03-28 21:54:08,345 : INFO : training on a 59383885 raw words (41974504 effective words) took 50.3s, 834417 effective words/s\n",
      "2018-03-28 21:54:08,348 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-03-28 21:54:08,465 : INFO : saving Word2Vec object under ./models/300features_40minwords_10context.model, separately None\n",
      "2018-03-28 21:54:08,467 : INFO : not storing attribute vectors_norm\n",
      "2018-03-28 21:54:08,469 : INFO : not storing attribute cum_table\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './models/300features_40minwords_10context.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-c5ae723bbdba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 存储模型，方便以后使用\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;31m# don't bother storing the cached normalized vectors, recalculable table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'vectors_norm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cum_table'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_latest_training_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/gensim/models/base_any2vec.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/gensim/utils.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved %s object\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# `fname_or_handle` does not have write attribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_smart_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparately\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/gensim/utils.pyc\u001b[0m in \u001b[0;36m_smart_save\u001b[0;34m(self, fname, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    548\u001b[0m                                        compress, subname)\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;31m# restore attribs handled specially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/gensim/utils.pyc\u001b[0m in \u001b[0;36mpickle\u001b[0;34m(obj, fname, protocol)\u001b[0m\n\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \"\"\"\n\u001b[0;32m-> 1311\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 'b' for binary, needed on Windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1312\u001b[0m         \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/smart_open/smart_open_lib.pyc\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_ERRORS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile_smart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"s3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"s3n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's3u'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ms3_open_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/smart_open/smart_open_lib.pyc\u001b[0m in \u001b[0;36mfile_smart_open\u001b[0;34m(fname, mode, encoding, errors)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mraw_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m     \u001b[0mraw_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0mdecompressed_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_fobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0mdecoded_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecompressed_fobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: './models/300features_40minwords_10context.model'"
     ]
    }
   ],
   "source": [
    "print('Traing model...')\n",
    "model = word2vec.Word2Vec(sentences,workers=num_workers,\n",
    "                 size=num_features,min_count=min_word_count,\n",
    "                 window=context,sample = downsampling\n",
    "                )\n",
    "# 初始化权重（映射矩阵）\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# 存储模型，方便以后使用\n",
    "model.save(os.path.join('.','models',model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 看看词向量训练的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitchen\n",
      "berlin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"man woman child kitchen\".split()))\n",
    "print(model.doesnt_match('france england germany berlin'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'mouse', 0.6986721158027649),\n",
       " (u'dog', 0.673465371131897),\n",
       " (u'sheep', 0.6436570286750793),\n",
       " (u'demon', 0.6173554062843323),\n",
       " (u'rabbit', 0.6109448671340942),\n",
       " (u'monkey', 0.6008049249649048),\n",
       " (u'lizard', 0.5990520715713501),\n",
       " (u'tiger', 0.5941593647003174),\n",
       " (u'bird', 0.5865774750709534),\n",
       " (u'doll', 0.5808638334274292)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangyanan/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'fbi', 0.6976989507675171),\n",
       " (u'authorities', 0.6819326281547546),\n",
       " (u'cops', 0.6687051057815552),\n",
       " (u'mob', 0.6635503172874451),\n",
       " (u'federal', 0.660423994064331),\n",
       " (u'cia', 0.6583893895149231),\n",
       " (u'terrorist', 0.6488812565803528),\n",
       " (u'investigating', 0.6443933248519897),\n",
       " (u'homicide', 0.6320115923881531),\n",
       " (u'agents', 0.6315895318984985)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('police')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
